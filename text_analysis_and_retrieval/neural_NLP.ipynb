{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77230ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3c4b605dcedd5b91abe5e1d914c2f67",
     "grade": false,
     "grade_id": "cell-cbdf7d65ea58446b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "University of Zagreb\\\n",
    "Faculty of Electrical Engineering and Computing\n",
    "\n",
    "## Text Analysis and Retrieval 2023/2024\n",
    "https://www.fer.unizg.hr/en/course/taar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee22e58",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1388d41fb10b43a183c74e7d5eb03486",
     "grade": false,
     "grade_id": "cell-5e9c1e104dec0dd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "------------------------------\n",
    "\n",
    "# LAB 3: Neural NLP\n",
    "\n",
    "*Version: 1.2*\n",
    "\n",
    "© 2024 Josip Jukić, Jan Šnajder\n",
    "\n",
    "Submission deadline: **April 21, 2024, 23:59 CET** \n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333437b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae77ac9def8dcb59dcc4f9c8a64ed69b",
     "grade": false,
     "grade_id": "cell-b25d76fa7c847af2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "Welcome, visitor! This lab assignment is structured into three segments. Your primary objective is to complete the missing code sections, marked by the \"YOUR CODE HERE\" placeholder, and then evaluate the cells.\n",
    "\n",
    "For each part of the assignment, a series of tests are available for you to run. These tests are designed to guide you by showing the expected output format. Additionally, after you submit your assignment, further tests will be conducted. Please note that variations in library versions might cause slight differences in your results. However, there's no need for concern, as your submitted work will be evaluated in a controlled environment.\n",
    "\n",
    "\n",
    "### Submission rules\n",
    "By submitting the exercise, you confirm the following points:\n",
    "1. You did not receive help from another when solving the exercise;\n",
    "2. You attributed parts of the code that were taken from the Internet by referencing them in comments;\n",
    "3. You did not use parts of the code from the Internet that are specific to the laboratory exercise;\n",
    "4. You have not used AI assistants for coding such as GitHub Copilot (including generative AI tools such as ChatGPT).\n",
    "\n",
    "**Violation of any of the above rules is considered a misdemeanor and results in academic sanctions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2336f31b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd1654b2c7c8de8e1d1cfaeeee261ec2",
     "grade": false,
     "grade_id": "cell-150c23ae802b9522",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52503faa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cf13460796faed6f774e768345186e9",
     "grade": false,
     "grade_id": "cell-95afad8333fec3bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1. Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e247b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c241add4f9218453930e1485763061e",
     "grade": false,
     "grade_id": "cell-7caecd650447b599",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Machine Translation (MT) involves the automatic conversion of text from one natural language to another, ensuring the original meaning is retained while generating coherent output in the target language. As one of the earliest areas of artificial intelligence research, machine translation has seen remarkable enhancements in quality, particularly with the adoption of large-scale empirical methods.\n",
    "\n",
    "In this lab assignment, we'll employ pre-trained sequence-to-sequence models for our machine translation tasks. We'll also explore two strategies for text generation: the greedy decoder and beam search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e673b2d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95325e1a7ab9f00340253c911831f8e3",
     "grade": false,
     "grade_id": "cell-9c4e4d599e282ec6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### (a)\n",
    "\n",
    "Assuming that we possess a pre-trained model, we still need to figure out how exactly we will generate tokens. One of the most straightforward approaches is to retrieve the most probable token in each step.\n",
    "Implement `greedy_decoder` for language generation. The greedy method retrieves the index of the most probable token for each timestep in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80305ded",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "834924716c12bf23b38e72cd7c43d350",
     "grade": false,
     "grade_id": "cell-2bc8e6ac47722f0f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def greedy_decoder(array):\n",
    "    \"\"\"\n",
    "    Retrieves a 1D numpy array containing the index of the most\n",
    "    probable token for each row.\n",
    "\n",
    "    Arguments:\n",
    "        array: 2D np.array of token probabilities, where each row correspond to\n",
    "        a certain timestamp. For example, 10x5 array represents a sequence of 10\n",
    "        words over a vocabulary of 5 words.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return np.argmax(array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb28c515",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e800457ec180902671743277b02cd02",
     "grade": true,
     "grade_id": "cell-e6919b45595c671e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ex1a1 = np.array(\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.5, 0.4],\n",
    "        [0.2, 0.1, 0.3, 0.4, 0.5],\n",
    "        [0.3, 0.4, 0.5, 0.2, 0.1],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.5, 0.3, 0.4, 0.2],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "sol1a1 = np.array([4, 0, 4, 0, 3, 4, 2, 0, 1, 0])\n",
    "\n",
    "assert (greedy_decoder(ex1a1) == sol1a1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca894a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1ae2ab666f197db3ecff1e7b440f622",
     "grade": false,
     "grade_id": "cell-37e0ec725ec6b044",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### (b)\n",
    "\n",
    "Greedy approach has the benefit that it is very fast, but the quality of the final output sequences may be far from optimal. Instead of greedily choosing the most likely next step as the sequence is constructed, the beam search expands all possible next steps and keeps the `k` most likely, where `k` is a parameter and controls the number of beams or parallel searches through the sequence of probabilities.\n",
    "\n",
    "The local beam search algorithm keeps track of `k` states rather than just one. It begins with `k` randomly generated states. At each step, all the successors of all `k` states are generated. If any one is a goal, the algorithm halts. Otherwise, it selects the k best successors from the complete list and repeats. This is illustrated in the figure below.\n",
    "\n",
    "Implement `beam_search_decoder`.\n",
    "\n",
    "![Beam search](img/beamsearch.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6f35d42",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69aea6d97cc973661ea365efd6df12cc",
     "grade": false,
     "grade_id": "cell-1daf6938107bcf6e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def beam_search_decoder(data, k):\n",
    "    \"\"\"\n",
    "    Retrieves top k sequences according to the beam search algorithm. The sequences\n",
    "    must be sorted in ascending order according to the sequence score.\n",
    "    If the sequence scores are the same, then use `str(numpy_array)` as the secondary sort key to resolve tie breaks,\n",
    "    where `numpy_array` is the corresponding probability sequence stored in a numpy array.\n",
    "\n",
    "    Scores for sequences are computed as the sum of negative logarithms. For example, the\n",
    "    scores for a sequence with probabilities [0.1, 0.3, 0.2] would be -log(0.1) - log(0.3) - log(0.2).\n",
    "\n",
    "    The result is returned as a sorted list of lists, where each list represents\n",
    "    one instance of the beam search output. For example, the list [[0, 1], [1, 2], [2, 3]] is\n",
    "    the result for k=3, where the first list [0, 1] (sequence of words from vocabulary at\n",
    "    position 0 and 1) is the output with the lowest score, [1, 2]\n",
    "    has the second lowest, and [2, 3] has the third lowest score.\n",
    "\n",
    "\n",
    "    Arguments:\n",
    "        array: 2D np.array of token probabilities, where each row corresponds to\n",
    "               a certain timestamp. For example, 10x5 array represents a sequence of 10\n",
    "               words over a vocabulary of 5 words.\n",
    "        k: number of beams\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    seqs = [[[], 1.0]]\n",
    "    for d in data:\n",
    "        all = []\n",
    "        for i in range(len(seqs)):\n",
    "            seq = seqs[i][0]\n",
    "            result = seqs[i][1]\n",
    "            for j in range(len(d)):\n",
    "                one = [seq+[j],result*-np.log(d[j])]\n",
    "                all.append(one)\n",
    "        seqs = sorted(all, key=lambda one: (one[1], str(np.array(one[0]))))[:k]\n",
    "    return list(map(lambda x: x[0], seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "077146a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbf6f92017793fb74daa78476daacfa1",
     "grade": true,
     "grade_id": "cell-2752c4f710125130",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ex1b1 = np.array(\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.5, 0.4],\n",
    "        [0.2, 0.1, 0.3, 0.4, 0.5],\n",
    "        [0.3, 0.4, 0.5, 0.2, 0.1],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.5, 0.3, 0.4, 0.2],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "sol1b1 = np.array(\n",
    "    [\n",
    "        [4, 0, 4, 0, 3, 4, 2, 0, 1, 0],\n",
    "        [3, 0, 4, 0, 3, 4, 2, 0, 1, 0],\n",
    "        [4, 0, 3, 0, 3, 4, 2, 0, 1, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert (beam_search_decoder(ex1b1, 3) == sol1b1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5026aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "645aaac19bf91b6222e7c576b6764bf9",
     "grade": false,
     "grade_id": "cell-be562f4995d3584e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### (c)\n",
    "\n",
    "Finally, let's employ a pre-trained sequence-to-sequence Transformer from the [`hugggingface`](https://huggingface.co/) library. Specifically, we are going to use `mBART-large-50`, which can be employed in the [zero-shot](https://en.wikipedia.org/wiki/Zero-shot_learning) setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85def3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# Load the model and its corresponding tokenizer.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57288b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07ad58e558933499e4b1b5e40188d73a",
     "grade": false,
     "grade_id": "cell-7247da3628b8c124",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Take a look at the example below to see how to use the pre-trained model in the zero-shot setup. We are translating from Finnish (fi_FI) to English (en_XX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc099eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The morning goes by in bed, and the day ends in reverse.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi_text = \"Aamu kuluu aatellessa, päivä päätä käännellessä.\"\n",
    "tokenizer.src_lang = \"fi_FI\"\n",
    "encoded_ar = tokenizer(fi_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(\n",
    "    **encoded_ar, max_length=50, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
    ")\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf1191",
   "metadata": {},
   "source": [
    "We can control the decoding algorithm by setting the `num_beams` parameter. If it is set to `None`, tokens will be decoded greedily. Additionally, we can control the sequence max length with `max_length` and early stopping with `early_stopping`. When set to True, `early_stopping` indicates that generation is finished when all beam hypotheses reached the EOS (end-of-sequence) token.\n",
    "\n",
    "Implement the `translate` method which generalizes translation using `mbart` to any source and target language supported by the model. If in doubt, refer to the previous example. Be sure to use all of the arguments, most of which are going to be forwarded to the model's `generate` method. `batch_decode` wraps the decoded tokens into a list, so don't forget to extract the first element from the list to get the actual string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12e4c8d0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9358c9fc4d132ba1d57e9a85785305b1",
     "grade": false,
     "grade_id": "cell-30758986b31cf01b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def translate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text,\n",
    "    src_lang,\n",
    "    tgt_lang=\"en_XX\",\n",
    "    num_beams=None,\n",
    "    max_length=50,\n",
    "    early_stopping=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Translates `text` from `src_lang` to `tgt_lang` and returns it as a string.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    tokenizer.src_lang = src_lang\n",
    "    encoded_ar = tokenizer(text, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(\n",
    "    **encoded_ar, max_length=max_length, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang],\n",
    "    num_beams=num_beams, early_stopping=early_stopping)\n",
    "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b102b",
   "metadata": {},
   "source": [
    "Translate Finnish (fi_FI) and Portugese (pt_PT) texts to English (see the cell below) using the `translate` method. Is greedy decoding doing OK or is it better to use beam search? If beam search is better, which `k` seems to perform well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "155e2861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger takes birds out of heaven and fish out of the sea.\n",
      "Greed takes birds out of heaven and fish out of the sea.\n"
     ]
    }
   ],
   "source": [
    "fi_text_1 = \"Ahneus vie linnut taivaalta ja kalat merestä.\"\n",
    "fi_text_2 = \"Aika on rahaa, sanoi työtön kun kellonsa myi.\"\n",
    "\n",
    "print(translate(model=model, tokenizer=tokenizer, text=fi_text_1, src_lang=\"fi_FI\", num_beams=1))\n",
    "print(translate(model=model, tokenizer=tokenizer, text=fi_text_1, src_lang=\"fi_FI\", num_beams=8))\n",
    "\n",
    "#print(translate(model=model, tokenizer=tokenizer, text=fi_text_2, src_lang=\"fi_FI\"))\n",
    "\n",
    "pt_text_1 = \"Mais vale um pássaro na mão do que dois voando.\"\n",
    "pt_text_2 = \"Diz-me com quem andas e eu te direi quem és.\"\n",
    "\n",
    "#print(translate(model=model, tokenizer=tokenizer, text=pt_text_1, src_lang=\"pt_PT\"))\n",
    "#print(translate(model=model, tokenizer=tokenizer, text=pt_text_2, src_lang=\"pt_PT\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e7fe29b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6fc0d5e39579a4429678e4c7312109f",
     "grade": true,
     "grade_id": "cell-00bb6ec8ed77cc00",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\38599\\miniconda3\\envs\\tar3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:444: UserWarning: `num_beams` is set to None - defaulting to 1.\n",
      "  warnings.warn(\"`num_beams` is set to None - defaulting to 1.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "ex1c1 = \"Anfangen ist leicht, Beharren ist Kunst\"\n",
    "assert translate(model, tokenizer, ex1c1, \"de_DE\") == \"Starting is easy, barking is art\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
